{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all required packages and modules.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import tensorflow as tf\n",
    "import transformers #huggingface transformers library\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sport</td>\n",
       "      <td>Roddick in talks over new coach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Prodigy join V Festival line-up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Sundance to honour foreign films</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>Dunne keen to commit to Man City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>politics</td>\n",
       "      <td>Row over 'police' power for CSOs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>1775</td>\n",
       "      <td>business</td>\n",
       "      <td>Lufthansa may sue over Bush visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>1776</td>\n",
       "      <td>tech</td>\n",
       "      <td>Rolling out next generation's net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>1777</td>\n",
       "      <td>sport</td>\n",
       "      <td>Mirza makes Indian tennis history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>1778</td>\n",
       "      <td>tech</td>\n",
       "      <td>GTA sequel is criminally good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>1779</td>\n",
       "      <td>tech</td>\n",
       "      <td>Go-ahead for new internet names</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1780 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id       Category                              Title\n",
       "0        0          sport    Roddick in talks over new coach\n",
       "1        1  entertainment    Prodigy join V Festival line-up\n",
       "2        2  entertainment   Sundance to honour foreign films\n",
       "3        3          sport   Dunne keen to commit to Man City\n",
       "4        4       politics   Row over 'police' power for CSOs\n",
       "...    ...            ...                                ...\n",
       "1775  1775       business  Lufthansa may sue over Bush visit\n",
       "1776  1776           tech  Rolling out next generation's net\n",
       "1777  1777          sport  Mirza makes Indian tennis history\n",
       "1778  1778           tech      GTA sequel is criminally good\n",
       "1779  1779           tech    Go-ahead for new internet names\n",
       "\n",
       "[1780 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load train and test data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['Title']\n",
    "test_X = test_data['Title']\n",
    "Y = train_data['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 1 ... 3 4 4]\n"
     ]
    }
   ],
   "source": [
    "#用label encoder將label轉成(0 1 2 3)\n",
    "le = LabelEncoder()\n",
    "le.fit(Y)\n",
    "Y = le.transform(Y)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(X)):\n",
    "#     #轉成小寫\n",
    "#     X[i] = X[i].lower()\n",
    "# for i in range(len(test_X)):\n",
    "#     #轉成小寫\n",
    "#     test_X[i] = test_X[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1780, 10)\n",
      "Shape of label tensor: (1780, 5)\n",
      "4081\n",
      "[[   0    0    0 ...    7    8  249]\n",
      " [   0    0    0 ...  250  307   13]\n",
      " [   0    0    0 ...  140  684  685]\n",
      " ...\n",
      " [   0    0    0 ...  285  608  892]\n",
      " [   0    0    0 ...   59 3622 1208]\n",
      " [   0    0    0 ...    8  478  501]]\n"
     ]
    }
   ],
   "source": [
    "train_text = np.array(X)\n",
    "test_text = np.array(test_X)\n",
    "train_text_dic = np.concatenate([train_text, test_text])\n",
    "MAX_SEQUENCE_LENGTH = 10\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text_dic)\n",
    "train_sequence = tokenizer.texts_to_sequences(train_text)\n",
    "word_index = tokenizer.word_index\n",
    "train_text_data = pad_sequences(train_sequence, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "train_text_labels = to_categorical(np.asarray(Y))\n",
    "print('Shape of data tensor:', train_text_data.shape)\n",
    "print('Shape of label tensor:', train_text_labels.shape)\n",
    "print(len(word_index))\n",
    "print(train_text_data)\n",
    "\n",
    "test_text = np.array(test_X)\n",
    "test_sequence = tokenizer.texts_to_sequences(test_text)\n",
    "#word_index = tokenizer.word_index\n",
    "test_text_data = pad_sequences(test_sequence, maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, EMBEDDING_DIM, attention, feed_forward_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = layers.MultiHeadAttention(num_heads=attention, key_dim=EMBEDDING_DIM)\n",
    "        self.feed_forward = Sequential([layers.Dense(feed_forward_dim, activation=\"relu\"), layers.Dense(EMBEDDING_DIM),])\n",
    "        self.normalization_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.normalization_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attention_output = self.attention(inputs, inputs)\n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        out1 = self.normalization_1(inputs + attention_output)\n",
    "        feed_forward_output = self.feed_forward(out1)\n",
    "        feed_forward_output = self.dropout2(feed_forward_output, training=training)\n",
    "        return self.normalization_2(out1 + feed_forward_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-88-cc3d0081b39d>:6: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  coefs = np.fromstring(coefs, 'f', sep=' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4529833 word vectors.\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 500\n",
    "embeddings_index = {}\n",
    "with open('enwiki_20180420_'+str(EMBEDDING_DIM)+'d.txt', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs        \n",
    "#Create glove embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "QQ=0\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word.lower())\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        QQ = QQ+1\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "#Keras Glove Embedding layer\n",
    "embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length = MAX_SEQUENCE_LENGTH, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 10, 500)           2041000   \n",
      "_________________________________________________________________\n",
      "transformer_block_9 (Transfo (None, 10, 500)           2038032   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_9 ( (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 4,081,537\n",
      "Trainable params: 4,081,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#attention head的數量\n",
    "num_heads = 2  \n",
    "#transformer中feed forward network的Hidden layer size \n",
    "feed_forward_dim = 32  \n",
    "inputs = layers.Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "x = embedding_layer(inputs)\n",
    "x = TransformerBlock(EMBEDDING_DIM, num_heads, feed_forward_dim)(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "model = Model(inputs=inputs, outputs = layers.Dense(5, activation=\"softmax\")(x))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 [==============================] - 21s 732ms/step - loss: 2.4231 - accuracy: 0.3975 - val_loss: 0.5464 - val_accuracy: 0.8399\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 16s 696ms/step - loss: 0.3740 - accuracy: 0.8722 - val_loss: 0.3960 - val_accuracy: 0.8652\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 16s 718ms/step - loss: 0.1070 - accuracy: 0.9645 - val_loss: 0.4052 - val_accuracy: 0.8848\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 17s 747ms/step - loss: 0.0349 - accuracy: 0.9941 - val_loss: 0.4574 - val_accuracy: 0.8792\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 16s 702ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.4610 - val_accuracy: 0.8876\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 16s 709ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.4746 - val_accuracy: 0.8820\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 16s 727ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.5635 - val_accuracy: 0.8820\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 16s 712ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4840 - val_accuracy: 0.8820\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 17s 720ms/step - loss: 4.1663e-04 - accuracy: 1.0000 - val_loss: 0.4983 - val_accuracy: 0.8848\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 30731s 1397s/step - loss: 2.7692e-04 - accuracy: 1.0000 - val_loss: 0.5088 - val_accuracy: 0.8848\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 10s 431ms/step - loss: 2.2115e-04 - accuracy: 1.0000 - val_loss: 0.5140 - val_accuracy: 0.8848\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 9s 395ms/step - loss: 2.2372e-04 - accuracy: 1.0000 - val_loss: 0.5180 - val_accuracy: 0.8848\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 8s 370ms/step - loss: 1.8988e-04 - accuracy: 1.0000 - val_loss: 0.5218 - val_accuracy: 0.8848\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 8s 354ms/step - loss: 1.5347e-04 - accuracy: 1.0000 - val_loss: 0.5275 - val_accuracy: 0.8848\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 8s 358ms/step - loss: 1.4070e-04 - accuracy: 1.0000 - val_loss: 0.5337 - val_accuracy: 0.8848\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 8s 354ms/step - loss: 1.1738e-04 - accuracy: 1.0000 - val_loss: 0.5382 - val_accuracy: 0.8848\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 9s 392ms/step - loss: 9.7942e-05 - accuracy: 1.0000 - val_loss: 0.5413 - val_accuracy: 0.8848\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 8s 364ms/step - loss: 1.0152e-04 - accuracy: 1.0000 - val_loss: 0.5446 - val_accuracy: 0.8848\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 8s 368ms/step - loss: 9.1445e-05 - accuracy: 1.0000 - val_loss: 0.5477 - val_accuracy: 0.8848\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 9s 371ms/step - loss: 7.7621e-05 - accuracy: 1.0000 - val_loss: 0.5525 - val_accuracy: 0.8848\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 16s 709ms/step - loss: 6.9628e-05 - accuracy: 1.0000 - val_loss: 0.5550 - val_accuracy: 0.8848\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 17s 720ms/step - loss: 8.1972e-05 - accuracy: 1.0000 - val_loss: 0.5586 - val_accuracy: 0.8848\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 16s 677ms/step - loss: 6.7630e-05 - accuracy: 1.0000 - val_loss: 0.5626 - val_accuracy: 0.8848\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 8s 353ms/step - loss: 5.9537e-05 - accuracy: 1.0000 - val_loss: 0.5644 - val_accuracy: 0.8848\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 8s 332ms/step - loss: 6.1879e-05 - accuracy: 1.0000 - val_loss: 0.5671 - val_accuracy: 0.8848\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 8s 332ms/step - loss: 5.4935e-05 - accuracy: 1.0000 - val_loss: 0.5696 - val_accuracy: 0.8848\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 8s 336ms/step - loss: 4.3154e-05 - accuracy: 1.0000 - val_loss: 0.5729 - val_accuracy: 0.8848\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 8s 328ms/step - loss: 4.7464e-05 - accuracy: 1.0000 - val_loss: 0.5762 - val_accuracy: 0.8848\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 8s 336ms/step - loss: 4.1143e-05 - accuracy: 1.0000 - val_loss: 0.5793 - val_accuracy: 0.8848\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 7s 327ms/step - loss: 4.1591e-05 - accuracy: 1.0000 - val_loss: 0.5821 - val_accuracy: 0.8848\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 8s 328ms/step - loss: 3.5004e-05 - accuracy: 1.0000 - val_loss: 0.5845 - val_accuracy: 0.8848\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 8s 329ms/step - loss: 3.1652e-05 - accuracy: 1.0000 - val_loss: 0.5871 - val_accuracy: 0.8848\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 7s 326ms/step - loss: 3.2697e-05 - accuracy: 1.0000 - val_loss: 0.5891 - val_accuracy: 0.8848\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 7s 326ms/step - loss: 3.2999e-05 - accuracy: 1.0000 - val_loss: 0.5905 - val_accuracy: 0.8848\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 7s 326ms/step - loss: 3.4307e-05 - accuracy: 1.0000 - val_loss: 0.5922 - val_accuracy: 0.8876\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 7s 326ms/step - loss: 3.4651e-05 - accuracy: 1.0000 - val_loss: 0.5948 - val_accuracy: 0.8876\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 7s 326ms/step - loss: 2.7216e-05 - accuracy: 1.0000 - val_loss: 0.5974 - val_accuracy: 0.8876\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 8s 328ms/step - loss: 2.7111e-05 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 0.8876\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 8s 328ms/step - loss: 2.7679e-05 - accuracy: 1.0000 - val_loss: 0.5999 - val_accuracy: 0.8876\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 8s 330ms/step - loss: 2.6059e-05 - accuracy: 1.0000 - val_loss: 0.6020 - val_accuracy: 0.8876\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 8s 328ms/step - loss: 2.4580e-05 - accuracy: 1.0000 - val_loss: 0.6038 - val_accuracy: 0.8876\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 7s 325ms/step - loss: 2.3372e-05 - accuracy: 1.0000 - val_loss: 0.6056 - val_accuracy: 0.8876\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 7s 326ms/step - loss: 2.3072e-05 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 0.8876\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 7s 326ms/step - loss: 1.9523e-05 - accuracy: 1.0000 - val_loss: 0.6099 - val_accuracy: 0.8876\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 8s 330ms/step - loss: 2.1710e-05 - accuracy: 1.0000 - val_loss: 0.6137 - val_accuracy: 0.8876\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 7s 326ms/step - loss: 1.9943e-05 - accuracy: 1.0000 - val_loss: 0.6150 - val_accuracy: 0.8876\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 7s 327ms/step - loss: 2.0504e-05 - accuracy: 1.0000 - val_loss: 0.6162 - val_accuracy: 0.8876\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 7s 326ms/step - loss: 1.6358e-05 - accuracy: 1.0000 - val_loss: 0.6181 - val_accuracy: 0.8876\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 8s 332ms/step - loss: 1.6524e-05 - accuracy: 1.0000 - val_loss: 0.6205 - val_accuracy: 0.8876\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 8s 353ms/step - loss: 1.5540e-05 - accuracy: 1.0000 - val_loss: 0.6215 - val_accuracy: 0.8876\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_text_data, train_text_labels, test_size=0.2, random_state=1, shuffle = False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.9999964e-01 1.8038696e-09 1.2611831e-10 1.0605586e-11 3.8500909e-07]\n",
      "['business', 'business', 'politics', 'tech', 'sport', 'business', 'politics', 'politics', 'tech', 'sport', 'sport', 'politics', 'entertainment', 'entertainment', 'politics', 'business', 'politics', 'entertainment', 'sport', 'tech', 'politics', 'business', 'sport', 'sport', 'tech', 'business', 'tech', 'sport', 'entertainment', 'sport', 'entertainment', 'tech', 'entertainment', 'tech', 'sport', 'business', 'entertainment', 'sport', 'politics', 'business', 'entertainment', 'sport', 'politics', 'business', 'business', 'politics', 'business', 'politics', 'politics', 'sport', 'tech', 'entertainment', 'sport', 'entertainment', 'sport', 'business', 'entertainment', 'politics', 'sport', 'tech', 'politics', 'sport', 'sport', 'politics', 'entertainment', 'sport', 'tech', 'politics', 'business', 'politics', 'politics', 'entertainment', 'business', 'tech', 'sport', 'tech', 'politics', 'business', 'sport', 'politics', 'tech', 'entertainment', 'politics', 'business', 'tech', 'sport', 'business', 'sport', 'tech', 'sport', 'politics', 'entertainment', 'sport', 'sport', 'tech', 'politics', 'sport', 'entertainment', 'entertainment', 'politics', 'business', 'business', 'sport', 'business', 'tech', 'entertainment', 'sport', 'tech', 'politics', 'politics', 'business', 'politics', 'politics', 'sport', 'business', 'entertainment', 'business', 'politics', 'tech', 'sport', 'sport', 'tech', 'sport', 'sport', 'tech', 'entertainment', 'entertainment', 'business', 'politics', 'entertainment', 'business', 'entertainment', 'tech', 'business', 'entertainment', 'business', 'business', 'business', 'sport', 'sport', 'tech', 'business', 'sport', 'tech', 'business', 'tech', 'politics', 'sport', 'politics', 'business', 'sport', 'politics', 'entertainment', 'entertainment', 'sport', 'sport', 'sport', 'entertainment', 'tech', 'tech', 'business', 'business', 'entertainment', 'entertainment', 'tech', 'politics', 'sport', 'entertainment', 'sport', 'entertainment', 'entertainment', 'entertainment', 'tech', 'entertainment', 'sport', 'sport', 'tech', 'tech', 'tech', 'business', 'tech', 'sport', 'business', 'sport', 'business', 'business', 'sport', 'entertainment', 'business', 'tech', 'business', 'tech', 'tech', 'tech', 'tech', 'business', 'politics', 'sport', 'sport', 'politics', 'sport', 'entertainment', 'politics', 'entertainment', 'sport', 'entertainment', 'tech', 'sport', 'politics', 'sport', 'entertainment', 'entertainment', 'sport', 'business', 'sport', 'tech', 'entertainment', 'politics', 'politics', 'business', 'business', 'tech', 'politics', 'entertainment', 'entertainment', 'sport', 'sport', 'sport', 'entertainment', 'sport', 'politics', 'politics', 'entertainment', 'politics', 'sport', 'entertainment', 'sport', 'politics', 'sport', 'business', 'entertainment', 'politics', 'politics', 'entertainment', 'tech', 'sport', 'sport', 'entertainment', 'sport', 'politics', 'sport', 'sport', 'politics', 'entertainment', 'business', 'tech', 'sport', 'sport', 'business', 'sport', 'business', 'business', 'business', 'sport', 'entertainment', 'entertainment', 'entertainment', 'sport', 'tech', 'tech', 'sport', 'politics', 'business', 'politics', 'politics', 'tech', 'sport', 'business', 'sport', 'entertainment', 'entertainment', 'politics', 'entertainment', 'business', 'entertainment', 'politics', 'tech', 'business', 'sport', 'business', 'politics', 'politics', 'business', 'business', 'politics', 'tech', 'politics', 'politics', 'sport', 'entertainment', 'sport', 'politics', 'entertainment', 'sport', 'business', 'tech', 'tech', 'politics', 'sport', 'entertainment', 'tech', 'politics', 'tech', 'tech', 'tech', 'sport', 'politics', 'tech', 'sport', 'politics', 'business', 'entertainment', 'politics', 'politics', 'tech', 'politics', 'tech', 'business', 'politics', 'sport', 'tech', 'business', 'entertainment', 'entertainment', 'sport', 'politics', 'business', 'business', 'tech', 'tech', 'entertainment', 'sport', 'politics', 'tech', 'politics', 'politics', 'tech', 'business', 'tech', 'tech', 'sport', 'tech', 'tech', 'sport', 'politics', 'entertainment', 'politics', 'sport', 'politics', 'sport', 'entertainment', 'sport', 'sport', 'tech', 'politics', 'business', 'business', 'tech', 'tech', 'business', 'sport', 'tech', 'politics', 'tech', 'tech', 'politics', 'tech', 'business', 'sport', 'tech', 'politics', 'business', 'business', 'sport', 'sport', 'politics', 'business', 'politics', 'politics', 'tech', 'business', 'entertainment', 'tech', 'sport', 'sport', 'business', 'tech', 'business', 'tech', 'business', 'entertainment', 'sport', 'sport', 'tech', 'business', 'sport', 'tech', 'business', 'politics', 'tech', 'politics', 'politics', 'politics', 'politics', 'entertainment', 'tech', 'entertainment', 'politics', 'sport', 'sport', 'sport', 'entertainment', 'business', 'politics', 'tech', 'tech', 'business', 'sport', 'politics', 'politics', 'sport', 'sport', 'sport', 'entertainment', 'entertainment', 'tech', 'sport', 'sport', 'business', 'politics', 'entertainment', 'tech', 'entertainment', 'sport', 'tech']\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_text_data)\n",
    "submit = []\n",
    "result = np.zeros(len(predictions))\n",
    "count = 0\n",
    "print(predictions[1])\n",
    "result = np.array([np.argmax(y, axis=None, out=None) for y in predictions])\n",
    "for pred in predictions:\n",
    "    p = pred.argmax()\n",
    "    submit.append(le.inverse_transform([p])[0])\n",
    "    count = count+1\n",
    "print(submit)\n",
    "submission = pd.DataFrame({'Id':test_data['Id'], 'Category':submit})\n",
    "submission.to_csv('309706019_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
